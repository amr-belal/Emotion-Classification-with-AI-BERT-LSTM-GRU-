
# 🧠 Emotion Classification using AI (BERT, LSTM, GRU, RNN)

🚀 **AI can now understand emotions!**  
This project implements **emotion classification** using **deep learning** and **transformers (BERT, LSTM, GRU, RNN)**. It analyzes text and predicts **six different emotions** using **Hugging Face’s Emotion Dataset**.

## 📌 Project Overview
- **Multiple AI models** trained: BERT (Transformers), LSTM, GRU, and SimpleRNN.
- **Dataset:** [Hugging Face Emotion Dataset](https://huggingface.co/datasets/emotion).
- **Evaluation Metrics:** Accuracy, Loss, and Visualization.
- **Deployment:** [Streamlit Web App](#-deployment).

---

## 📊 Dataset
We used the **Hugging Face Emotion Dataset**, which contains **16,000+ training samples** labeled with six emotions:

✅ **0 - Sadness**  
✅ **1 - Joy**  
✅ **2 - Love**  
✅ **3 - Anger**  
✅ **4 - Fear**  
✅ **5 - Surprise**  

---

## ⚙️ Project Structure

---

## 🚀 Model Architectures

### 🔹 **1. Transformer-Based Model (BERT)**
BERT uses deep bidirectional transformers to capture contextual meaning.

```python
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer

bert_model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=6)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

