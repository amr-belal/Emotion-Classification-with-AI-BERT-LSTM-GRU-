
# ğŸ§  Emotion Classification using AI (BERT, LSTM, GRU, RNN)

ğŸš€ **AI can now understand emotions!**  
This project implements **emotion classification** using **deep learning** and **transformers (BERT, LSTM, GRU, RNN)**. It analyzes text and predicts **six different emotions** using **Hugging Faceâ€™s Emotion Dataset**.

## ğŸ“Œ Project Overview
- **Multiple AI models** trained: BERT (Transformers), LSTM, GRU, and SimpleRNN.
- **Dataset:** [Hugging Face Emotion Dataset](https://huggingface.co/datasets/emotion).
- **Evaluation Metrics:** Accuracy, Loss, and Visualization.
- **Deployment:** [Streamlit Web App](#-deployment).

---

## ğŸ“Š Dataset
We used the **Hugging Face Emotion Dataset**, which contains **16,000+ training samples** labeled with six emotions:

âœ… **0 - Sadness**  
âœ… **1 - Joy**  
âœ… **2 - Love**  
âœ… **3 - Anger**  
âœ… **4 - Fear**  
âœ… **5 - Surprise**  

---

## âš™ï¸ Project Structure

---

## ğŸš€ Model Architectures

### ğŸ”¹ **1. Transformer-Based Model (BERT)**
BERT uses deep bidirectional transformers to capture contextual meaning.

```python
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer

bert_model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=6)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

